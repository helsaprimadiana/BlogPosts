{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "import math\n",
    "from dask.multiprocessing import get\n",
    "from dask.distributed import Client, LocalCluster \n",
    "print('test')\n",
    "import dask\n",
    "\n",
    "#cluster = LocalCluster(n_workers = 4, threads_per_worker = 4, processes=False, memory_limit='4GB', silence_logs='error')\n",
    "client = Client(processes=False, silence_logs='error')\n",
    "#client = Client(cluster)\n",
    "#cluster\n",
    "client\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna - used because null/None is float, not str, had to make it a str, for max/min\n",
    "def get_inventory(file_type:str) -> dd:\n",
    "    if file_type == 'parquet':\n",
    "        df = dd.read_parquet('./data/checkouts/inventory_parquet', engine=\"pyarrow\")\n",
    "    elif 'csv' in file_type:\n",
    "        df = (dd\n",
    "          .read_csv(\"./data/checkouts/Library_Collection_Inventory.csv\",\n",
    "                    header=0, \n",
    "                    dtype={\"BibNum\":\"int64\", \"Title\":\"str\",\"Author\":\"str\", \"Subjects\":\"str\"})\n",
    "         )  \n",
    "    df = df.drop_duplicates(subset='BibNum')\n",
    "    return df\n",
    "\n",
    "def get_inventory_no_dedup(file_type:str) -> dd:\n",
    "    if file_type == 'parquet':\n",
    "        df = dd.read_parquet('./data/checkouts/inventory_parquet', engine=\"pyarrow\")\n",
    "    elif 'csv' in file_type:\n",
    "        df = (dd\n",
    "          .read_csv(\"./data/checkouts/Library_Collection_Inventory.csv\",\n",
    "                    header=0, \n",
    "                    dtype={\"BibNum\":\"int64\", \"Title\":\"str\",\"Author\":\"str\", \"Subjects\":\"str\"})\n",
    "         )  \n",
    "    return df\n",
    "    \n",
    "\n",
    "def get_checkouts(file_type:str) -> dd:\n",
    "    if file_type == 'multi-csv':\n",
    "        df = dd.read_csv(\"./data/checkouts/Checkouts_By_Title_Data_Lens_*.csv\", \n",
    "                         header=0, \n",
    "                         dtype={\"BibNumber\":'int64',\n",
    "                                \"ItemBarcode\":\"str\",\n",
    "                                \"ItemType\":\"str\",\n",
    "                                \"Collection\":\"str\",\n",
    "                                \"CallNumber\":\"str\",\n",
    "                                \"CheckoutDateTime\":\"str\"}\n",
    "                        )\n",
    "    elif file_type =='single-csv':\n",
    "        df = dd.read_csv(\"./data/checkouts/Checkouts.csv\", \n",
    "                         header=0, \n",
    "                         dtype={\"BibNumber\":'int64',\n",
    "                                \"ItemBarcode\":\"str\",\n",
    "                                \"ItemType\":\"str\",\n",
    "                                \"Collection\":\"str\",\n",
    "                                \"CallNumber\":\"str\",\n",
    "                                \"CheckoutDateTime\":\"str\"}\n",
    "                        )\n",
    "    elif file_type =='parquet':\n",
    "        df = dd.read_parquet('./data/checkouts/checkouts_parquet', engine=\"pyarrow\")  \n",
    "    return df\n",
    "\n",
    "def writter_csv(df: dd, scheduler_type, single_file: bool) -> None:\n",
    "    if single_file:\n",
    "        df.to_csv('./output/output.csv',single_file=single_file, compute_kwargs={\"scheduler\": scheduler_type}, mode=\"w+\")\n",
    "    else:\n",
    "        df.to_csv('./output/output',single_file=single_file, compute_kwargs={\"scheduler\": scheduler_type}, mode=\"w+\")\n",
    "        \n",
    "def writter_parquet(df: dd, scheduler, flag=None) ->None:\n",
    "    df.to_parquet(\"./output/parquet_output\")\n",
    "    \n",
    "def add_formatted_checkout(file_type:str)->dd:\n",
    "    df = get_checkouts(file_type)\n",
    "    df['CheckoutTime_formated']=dd.to_datetime(df.CheckoutDateTime, format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    return df\n",
    "def add_formatted_checkout_map_partitions(file_type:str)->dd:\n",
    "    df = get_checkouts(file_type)\n",
    "    df['CheckoutTime_formated']=df.CheckoutDateTime.map_partitions(pd.to_datetime, \n",
    "                                               format=\"%m/%d/%Y %I:%M:%S %p\"\n",
    "                                                           )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "computes = [\"threads\", \"processes\",\"synchronous\"]\n",
    "file_types = [\"parquet\", \"multi-csv\", \"single-csv\"]\n",
    "wr = [[writter_csv, False], [writter_parquet,False],[writter_csv, True]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.9 ms ± 5.87 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "77.9 ms ± 4.63 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "84.2 ms ± 1.54 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 5 get_checkouts(\"parquet\")\n",
    "%timeit -n 1 -r 5 get_checkouts(\"multi-csv\")\n",
    "%timeit -n 1 -r 5 get_checkouts(\"single-csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting head of read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19 s ± 39.4 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "1.75 s ± 173 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "1.6 s ± 63.3 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 5 get_checkouts(\"parquet\").head()\n",
    "%timeit -n 1 -r 5 get_checkouts(\"multi-csv\").head()\n",
    "%timeit -n 1 -r 5 get_checkouts(\"single-csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading small file and writting to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compute in computes:\n",
    "    for file_type in file_types:\n",
    "        for writter in wr:\n",
    "            print(f\"read + write. compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "            %timeit -n 1 -r 5 writter[0](get_inventory_no_filter(file_type), compute, writter[1])\n",
    "            print(f\"Read, dedup, write. compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "            %timeit -n 1 -r 5 writter[0](get_inventory(file_type), compute, writter[1])\n",
    "            print(f\"Checkouts Read + Write. compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "            %timeit -n 1 -r 5 writter[0](get_checkouts(file_type), compute, writter[1])\n",
    "            print(f\"Add timestamp and write: compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "            %timeit -n 1 -r 5 writter[0](add_formatted_checkout(file_type),compute, writter[1])\n",
    "            print(f\"add timestamp with map partitions + write: compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "            %timeit -n 1 -r 5 writter[0](add_formatted_checkout_map_partitions(file_type),compute, writter[1])\n",
    "            print(f\"Get row count: compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "            %timeit -n 1 -r 5 get_checkouts(file_type).count().compute()\n",
    "            print(f\"add timestamp, get first 10: compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "            %timeit -n 1 -r 5 add_formatted_checkout(file_type).nsmallest(10, \"CheckoutTime_formated\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read convert to datetime, get min datetime value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compute in computes:\n",
    "    for file_type in file_types:\n",
    "        stop_list = []\n",
    "        for i in range(5):\n",
    "            start = timeit.default_timer()\n",
    "            df = add_formatted_checkout(file_type)\n",
    "            df = df.CheckoutTime_formated.min()\n",
    "            df = dask.optimize(df)[0]\n",
    "            print(df.compute(scheduler=compute))\n",
    "            stop = timeit.default_timer()\n",
    "            stop_list = stop_list.append(stop-start)\n",
    "        print(f\"compute: {compute}, file_type: {file_type}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compute in computes:\n",
    "    for file_type in file_types:\n",
    "        stop_list = []\n",
    "        for i in range(5):\n",
    "            start = timeit.default_timer()\n",
    "            df = add_formatted_checkout_map_partitions(file_type)\n",
    "            df = df.CheckoutTime_formated.min()\n",
    "            df = dask.optimize(df)[0]\n",
    "            print(df.compute(scheduler=compute))\n",
    "            stop = timeit.default_timer()\n",
    "            stop_list = stop_list.append(stop-start)\n",
    "        print(f\"compute: {compute}, file_type: {file_type}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner Join and head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_type in file_types:\n",
    "    stop_list = []\n",
    "    for i in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        checkouts = get_checkouts(\"parquet\")[[\"BibNumber\", \"CheckoutDateTime\"]]\n",
    "        inventory = get_inventory(\"parquet\")[[\"BibNum\", \"Author\"]]\n",
    "        merged = dd.merge(checkouts, inventory, left_on=\"BibNumber\", right_on=\"BibNum\", how=\"inner\")\n",
    "        merged.head()\n",
    "        stop = timeit.default_timer()\n",
    "        stop_list = stop_list.append(stop-start)\n",
    "    print(f\"file_type: {file_type}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anti join and head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_type in file_types:\n",
    "    stop_list = []\n",
    "    for i in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        checkouts = get_checkouts(\"parquet\")[[\"BibNumber\", \"CheckoutDateTime\"]]\n",
    "        inventory = get_inventory(\"parquet\")[[\"BibNum\", \"Author\"]]\n",
    "        merged = dd.merge(checkouts, inventory, left_on=\"BibNumber\", right_on=\"BibNum\", how=\"outer\", indicator=True)\n",
    "        merged[merged._merge==\"right_only\"].head()\n",
    "        stop = timeit.default_timer()\n",
    "        stop_list = stop_list.append(stop-start)\n",
    "    print(f\"file_type: {file_type}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read + explode + write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compute in computes:\n",
    "    for file_type in file_types:\n",
    "        for writter in wr:\n",
    "            stop_list = []\n",
    "            for i in range(5):\n",
    "                start = timeit.default_timer()\n",
    "                inventory = get_inventory(file_type)[[\"BibNum\", \"Subjects\"]]\n",
    "                inventory['Subject'] = inventory['Subjects'].str.split(',').fillna(\"\")\n",
    "                inventory = inventory.explode(\"Subject\")\n",
    "                inventory = dask.optimize(inventory)[0]\n",
    "                writter[0](inventory, compute, writter[1])\n",
    "                stop = timeit.default_timer()\n",
    "                stop_list = stop_list.append(stop-start)\n",
    "            print(f\"compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read+explode+dcount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_type in file_types:\n",
    "    stop_list = []\n",
    "    for i in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        inventory = get_inventory(file_type)[[\"BibNum\", \"Subjects\"]]\n",
    "        inventory['Subject'] = inventory['Subjects'].str.split(',').fillna(\"\")\n",
    "        inventory = inventory.explode(\"Subject\")\n",
    "        inventory = inventory.map_partitions(lambda x: x.groupby(x.Subject)[\"BibNum\"].nunique().astype(\"int64\"))\n",
    "        inventory = dask.optimize(inventory)[0]\n",
    "        inventory.head()\n",
    "        stop = timeit.default_timer()\n",
    "        stop_list = stop_list.append(stop-start)\n",
    "    print(f\"compute: {compute}, file_type: {file_type}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_type in file_types:\n",
    "    stop_list = []\n",
    "    for i in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        inventory = get_inventory(file_type)[[\"BibNum\", \"Subjects\"]]\n",
    "        inventory['Subject'] = inventory['Subjects'].str.split(',').fillna(\"\")\n",
    "        inventory = inventory.explode(\"Subject\")\n",
    "        inventory = inventory.groupby(inventory.Subject)[\"BibNum\"].nunique().astype(\"int64\"))\n",
    "        inventory = dask.optimize(inventory)[0]\n",
    "        inventory.head()\n",
    "        stop = timeit.default_timer()\n",
    "        stop_list = stop_list.append(stop-start)\n",
    "    print(f\"compute: {compute}, file_type: {file_type}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read + explode + join + dcount + head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_type in file_types:\n",
    "    stop_list = []\n",
    "    for i in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        checkouts = get_checkouts(\"parquet\")[[\"BibNumber\", \"CheckoutDateTime\"]]\n",
    "        inventory = get_inventory(file_type)[[\"BibNum\", \"Subjects\"]]\n",
    "        inventory['Subject'] = inventory['Subjects'].str.split(',').fillna(\"\")\n",
    "        inventory = inventory.explode(\"Subject\")\n",
    "        merged = dd.merge(checkouts, inventory, left_on=\"BibNumber\", right_on=\"BibNum\", how=\"inner\")\n",
    "        merged = merged.map_partitions(lambda x: x.groupby(x.Subject)[\"BibNum\"].nunique().astype(\"int64\"))\n",
    "        merged = dask.optimize(merged)[0]\n",
    "        merged.head()\n",
    "        stop = timeit.default_timer()\n",
    "        stop_list = stop_list.append(stop-start)\n",
    "    print(f\"compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_type in file_types:\n",
    "    stop_list = []\n",
    "    for i in range(5):\n",
    "        start = timeit.default_timer()\n",
    "        checkouts = get_checkouts(\"parquet\")[[\"BibNumber\", \"CheckoutDateTime\"]]\n",
    "        inventory = get_inventory(file_type)[[\"BibNum\", \"Subjects\"]]\n",
    "        inventory['Subject'] = inventory['Subjects'].str.split(',').fillna(\"\")\n",
    "        inventory = inventory.explode(\"Subject\")\n",
    "        merged = dd.merge(checkouts, inventory, left_on=\"BibNumber\", right_on=\"BibNum\", how=\"inner\")\n",
    "        merged = merged.groupby(merged.Subject)[\"BibNum\"].nunique().astype(\"int64\")\n",
    "        merged = dask.optimize(merged)[0]\n",
    "        merged.head()\n",
    "        stop = timeit.default_timer()\n",
    "        stop_list = stop_list.append(stop-start)\n",
    "    print(f\"compute: {compute}, file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}, runtime: {sum(stop_list)/len(stop_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
