{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-311c988bd015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountDistinct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munix_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_unixtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDDBarrier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBroadcastPickleRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/accumulators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msocketserver\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSocketServer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPickleSerializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mxrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_exception_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0m_cell_set_template_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_cell_set_template_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         return types.CodeType(\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_kwonlyargcount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type bytes)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "from pyspark.sql.functions import col, to_timestamp, date_format, countDistinct, unix_timestamp, from_unixtime, count, asc, desc, split, explode, min, max\n",
    "import argparse\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "import glob\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "            SparkSession.builder.appName(\"Spark Benchmarking\")\n",
    "            .master(\"local[*]\")\n",
    "            .config('spark.driver.host', '127.0.0.1')\n",
    "            .config(\"spark.driver.memory\", \"16g\")\n",
    "            .config(\"spark.driver.maxResultSize\", \"16g\")\n",
    "            .getOrCreate()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inventory(spark: SparkSession, file_type:str) -> SparkDataFrame:\n",
    "    df = \"\"\n",
    "    if file_type == 'parquet':\n",
    "        return spark.read.parquet(\"./data/checkouts/inventory_parquet\").select(\"BibNum\", \"Title\",\"Author\", \"Subjects\").dropDuplicates(['BibNum'])\n",
    "    else:\n",
    "        return spark.read.option(\"header\",\"true\").csv(\"./data/checkouts/Library_Collection_Inventory.csv\").select(\"BibNum\", \"Title\",\"Author\", \"Subjects\").dropDuplicates(['BibNum'])\n",
    "\n",
    "def get_checkouts(spark: SparkSession, file_type:str) -> SparkDataFrame:\n",
    "    df = ''\n",
    "    if file_type == 'multi-csv':\n",
    "        return spark.read.option(\"header\",\"true\").csv(\"./data/checkouts/Checkouts_By_Title_Data_Lens_*.csv\")\n",
    "    elif file_type == 'single-csv':\n",
    "        return spark.read.option(\"header\",\"true\").csv(\"./data/checkouts/Checkouts.csv\")\n",
    "    else:\n",
    "        return spark.read.option(\"header\",\"true\").parquet(\"./data/checkouts/checkouts_parquet\")\n",
    "\n",
    "\n",
    "def writter_csv(df: SparkDataFrame, single_file: bool) -> None:\n",
    "    if single_file:\n",
    "        df.coalesce(1).write.format('csv').mode(\"overwrite\").save(f\"./output/output_folder\")\n",
    "        for filename in glob.glob('./output/output_folder/*.csv'):\n",
    "            shutil.move(filename, './output/output.csv')\n",
    "    else:\n",
    "        df.write.format('csv').mode(\"overwrite\").save(f\"./output/output_folder\")\n",
    "        \n",
    "def writter_parquet(df: SparkDataFrame, single_file: bool) ->None:\n",
    "    df.write.format(\"parquet\").mode(\"overwrite\").save(f\"./output/parquet_output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_types = [\"parquet\", \"multi-csv\", \"single-csv\", ]\n",
    "wr = [[writter_parquet,False], [writter_csv, False], [writter_csv, True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario1(writter, single_file, file_type):\n",
    "    df = get_checkouts(spark, file_type)\n",
    "    writter(df, single_file)\n",
    "#Read and write\n",
    "def scenario1_5(writter, single_file, file_type):\n",
    "    df = get_inventory(spark, file_type)\n",
    "    writter(df, single_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkouts. file_type: parquet, writter: writter_parquet, single_file = False\n",
      "1min 1s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: parquet, writter: writter_parquet, single_file = False\n",
      "8.17 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: parquet, writter: writter_csv, single_file = False\n",
      "57.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: parquet, writter: writter_csv, single_file = False\n",
      "7.73 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: parquet, writter: writter_csv, single_file = True\n",
      "2min 2s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: parquet, writter: writter_csv, single_file = True\n",
      "8.76 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: multi-csv, writter: writter_parquet, single_file = False\n",
      "1min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: multi-csv, writter: writter_parquet, single_file = False\n",
      "6.08 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: multi-csv, writter: writter_csv, single_file = False\n",
      "58.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: multi-csv, writter: writter_csv, single_file = False\n",
      "5.81 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: multi-csv, writter: writter_csv, single_file = True\n",
      "3min 41s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: multi-csv, writter: writter_csv, single_file = True\n",
      "7.89 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: single-csv, writter: writter_parquet, single_file = False\n",
      "1min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: single-csv, writter: writter_parquet, single_file = False\n",
      "8.34 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: single-csv, writter: writter_csv, single_file = False\n",
      "1min 4s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: single-csv, writter: writter_csv, single_file = False\n",
      "6.37 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Checkouts. file_type: single-csv, writter: writter_csv, single_file = True\n",
      "4min 8s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Inventory. file_type: single-csv, writter: writter_csv, single_file = True\n",
      "10.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for file_type in file_types:\n",
    "    for writter in wr:\n",
    "        print(f\"Checkouts. file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "        %timeit -n 1 -r 1 scenario1(writter[0], writter[1], file_type)\n",
    "        print(f\"Inventory. file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "        %timeit -n 1 -r 1 scenario1_5(writter[0], writter[1], file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_formatted_checkout(spark:SparkSession, file_type:str)->SparkDataFrame:\n",
    "    df = get_checkouts(spark, file_type)\n",
    "    df = df.withColumn(\"CheckoutTime_formated\", to_timestamp(from_unixtime(unix_timestamp(col(('CheckoutDateTime')), \"MM/dd/yyyy hh:mm:ss aa\"))))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read add a column and write\n",
    "def scenario2(writter, single_flag, file_type):\n",
    "    df = add_formatted_checkout(spark,file_type)\n",
    "    writter(df, single_flag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type: parquet, writter: writter_parquet, single_file = False\n",
      "2min 33s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = False\n",
      "2min 27s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = True\n",
      "7min 40s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_parquet, single_file = False\n",
      "2min 59s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = False\n",
      "2min 44s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = True\n",
      "10min 25s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_parquet, single_file = False\n",
      "2min 57s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = False\n",
      "2min 54s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = True\n",
      "11min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for file_type in file_types:\n",
    "    for writter in wr:\n",
    "        print(f\"file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "        %timeit -n 1 -r 1 scenario2(writter[0], writter[1], file_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read add a column and write\n",
    "def scenario3(writter, single_flag, file_type):\n",
    "    df = add_formatted_checkout(spark,file_type)\n",
    "    df = df.withColumn('month', date_format('CheckoutTime_formated','yyyy-MM'))\n",
    "    df.groupby('month').count().collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type: parquet, writter: writter_parquet, single_file = False\n",
      "1min 40s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = False\n",
      "1min 35s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = True\n",
      "1min 33s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_parquet, single_file = False\n",
      "2min 8s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = False\n",
      "2min 5s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = True\n",
      "2min 10s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_parquet, single_file = False\n",
      "2min 7s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = False\n",
      "2min 8s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = True\n",
      "2min 6s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for file_type in file_types:\n",
    "    for writter in wr:\n",
    "        print(f\"file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "        %timeit -n 1 -r 1 scenario3(writter[0], writter[1], file_type)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type: parquet\n",
      "94194815\n",
      "240 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv\n",
      "91980693\n",
      "9.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv\n",
      "91980693\n",
      "9.83 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# read and get row count Scenario3\n",
    "for file_type in file_types:\n",
    "    print(f\"file_type: {file_type}\")\n",
    "    %timeit -n 1 -r 1 print(get_checkouts(spark, file_type).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type: parquet\n",
      "[Row(min(CheckoutTime_formated)=datetime.datetime(2005, 4, 13, 8, 0))]\n",
      "1min 51s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv\n",
      "[Row(min(CheckoutTime_formated)=datetime.datetime(2005, 4, 13, 8, 0))]\n",
      "1min 39s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv\n",
      "[Row(min(CheckoutTime_formated)=datetime.datetime(2005, 4, 13, 8, 0))]\n",
      "1min 33s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#read, column transform + min value of new column compute\n",
    "for file_type in file_types:\n",
    "    print(f\"file_type: {file_type}\")\n",
    "    %timeit -n 1 -r 1 print(add_formatted_checkout(spark, file_type).select(min(\"CheckoutTime_formated\")).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read, join, write\n",
    "def scenario4(writter, single_flag, file_type):\n",
    "    checkouts = get_checkouts(spark, file_type).select(\"BibNumber\",\"ItemBarcode\")\n",
    "    inventory = get_inventory(spark, file_type).select(\"BibNum\", \"Author\")\n",
    "    merged = checkouts.join(inventory, inventory.BibNum==checkouts.BibNumber, how=\"inner\")\n",
    "    # row count of merge: 62545918\n",
    "    writter(merged, single_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type: parquet, writter: writter_parquet, single_file = False\n",
      "29.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = False\n",
      "29.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = True\n",
      "1min 40s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_parquet, single_file = False\n",
      "1min 5s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = False\n",
      "1min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = True\n",
      "2min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_parquet, single_file = False\n",
      "1min 3s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = False\n",
      "1min 8s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = True\n",
      "2min 30s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for file_type in file_types:\n",
    "    for writter in wr:\n",
    "        print(f\"file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "        %timeit -n 1 -r 1 scenario4(writter[0], writter[1], file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read, join, group by + count , compute top 10, \n",
    "def scenario5(writter, single_flag, file_type):\n",
    "    checkouts = get_checkouts(spark, file_type).select(\"BibNumber\",\"ItemBarcode\")\n",
    "    inventory = get_inventory(spark, file_type).select(\"BibNum\", \"Author\")\n",
    "    merged = checkouts.join(inventory, inventory.BibNum==checkouts.BibNumber, how=\"inner\")\n",
    "    merged = (merged.select(\"BibNum\", \"Author\")\n",
    "              .groupby('Author')\n",
    "              .agg(count('BibNum').alias(\"times_taken\"))\n",
    "              .orderBy(desc(\"times_taken\")).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type: parquet, writter: writter_csv, single_file = True\n",
      "1min 43s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = True\n",
      "2min 29s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = True\n",
      "2min 30s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for file_type in file_types:\n",
    "    print(f\"file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "    %timeit -n 1 -r 1 scenario4(writter[0], writter[1], file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read, string split, explode, write\n",
    "def scenario7(writter, single_flag, file_type):\n",
    "    inventory = get_inventory(spark, file_type).select(\"BibNum\", \"Subjects\")\n",
    "    inventory = inventory.withColumn(\"Subject\", explode(split(col(\"Subjects\"), \",\").alias(\"Subject\")))\n",
    "    inventory = inventory.groupby('Subject').agg(countDistinct('BibNum').alias(\"subject_occurence\"))\n",
    "    writter(inventory, single_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type: parquet, writter: writter_parquet, single_file = False\n",
      "10.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = False\n",
      "9.72 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: parquet, writter: writter_csv, single_file = True\n",
      "12.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_parquet, single_file = False\n",
      "11.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = False\n",
      "11.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: multi-csv, writter: writter_csv, single_file = True\n",
      "12.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_parquet, single_file = False\n",
      "11.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = False\n",
      "11.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "file_type: single-csv, writter: writter_csv, single_file = True\n",
      "12.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "for file_type in file_types:\n",
    "    for writter in wr:\n",
    "        print(f\"file_type: {file_type}, writter: {writter[0].__name__}, single_file = {writter[1]}\")\n",
    "        %timeit -n 1 -r 1 scenario7(writter[0], writter[1], file_type)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",\"true\").csv(\"./data/checkouts/Library_Collection_Inventory.csv\").write.format(\"parquet\").mode(\"overwrite\").save(f\"./data/checkouts/inventory_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584391"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_inventory(spark, \"parquet\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
